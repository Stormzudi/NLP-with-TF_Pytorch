{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10RYw2RphUPP"
   },
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSDBp0S7hSWp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpnzkORUhbQ3"
   },
   "source": [
    "# 自定义数据和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvqGRtpThfUO"
   },
   "outputs": [],
   "source": [
    "# 3 words sentences (=sequence_length is 3)\n",
    "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "# TextCNN Parameter\n",
    "embedding_size = 2\n",
    "sequence_length = len(sentences[0].split()) # every sentences contains sequence_length(=3) words\n",
    "num_classes = len(set(labels))  # num_classes=2\n",
    "batch_size = 3\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "vocab = list(set(word_list))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp0Fy-Y2jzw3"
   },
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTWkBZ5Aj1Av"
   },
   "outputs": [],
   "source": [
    "def make_data(sentences, labels):\n",
    "  inputs = []\n",
    "  for sen in sentences: # sen是一个句子\n",
    "      inputs.append([word2idx[n] for n in sen.split()])\n",
    "\n",
    "  targets = []\n",
    "  for out in labels:\n",
    "      targets.append(out) # To using Torch Softmax Loss function\n",
    "  return inputs, targets\n",
    "\n",
    "input_batch, target_batch = make_data(sentences, labels)\n",
    "input_batch, target_batch = torch.LongTensor(input_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "dataset = Data.TensorDataset(input_batch, target_batch)\n",
    "loader = Data.DataLoader(dataset, batch_size, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4_K8BODk-hm"
   },
   "source": [
    "# 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pMEVcc3k-_-"
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.W = nn.Embedding(vocab_size, embedding_size)\n",
    "        output_channel = 3\n",
    "        self.conv = nn.Sequential(\n",
    "            # conv : [input_channel(=1), output_channel, (filter_height, filter_width), stride=1]\n",
    "            nn.Conv2d(1, output_channel, (2, embedding_size)),\n",
    "            nn.ReLU(),\n",
    "            # pool : ((filter_height, filter_width))\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "        )\n",
    "        # fc\n",
    "        self.fc = nn.Linear(output_channel, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "      '''\n",
    "      X: [batch_size, sequence_length]\n",
    "      '''\n",
    "      batch_size = X.shape[0]\n",
    "      embedding_X = self.W(X) # [batch_size, sequence_length, embedding_size]\n",
    "      embedding_X = embedding_X.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n",
    "      conved = self.conv(embedding_X) # [batch_size, output_channel*1*1]\n",
    "      flatten = conved.view(batch_size, -1)\n",
    "      output = self.fc(flatten)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJmL5PYRlthn"
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wG30S4Cylv3v",
    "outputId": "886247af-91e6-4516-ec3d-39f62564bf07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 loss = 0.136996\n",
      "Epoch: 1000 loss = 0.501182\n",
      "Epoch: 2000 loss = 0.636514\n",
      "Epoch: 2000 loss = 0.000245\n",
      "Epoch: 3000 loss = 0.135205\n",
      "Epoch: 3000 loss = 0.501388\n",
      "Epoch: 4000 loss = 0.501312\n",
      "Epoch: 4000 loss = 0.135233\n",
      "Epoch: 5000 loss = 0.501346\n",
      "Epoch: 5000 loss = 0.135189\n"
     ]
    }
   ],
   "source": [
    "model = TextCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "for epoch in range(5000):\n",
    "  for batch_x, batch_y in loader:\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "    pred = model(batch_x)\n",
    "    loss = criterion(pred, batch_y)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH66BgBulydv"
   },
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MBsasvHlz5u",
    "outputId": "0395d0d0-3114-4d07-97a7-3453bf43fab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hate me is Bad Mean...\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_text = 'i hate me'\n",
    "tests = [[word2idx[n] for n in test_text.split()]]\n",
    "test_batch = torch.LongTensor(tests).to(device)\n",
    "# Predict\n",
    "model = model.eval()\n",
    "predict = model(test_batch).data.max(1, keepdim=True)[1] # 返回最大的元素并返回索引\n",
    "if predict[0][0] == 0:\n",
    "    print(test_text,\"is Bad Mean...\")\n",
    "else:\n",
    "    print(test_text,\"is Good Mean!!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
